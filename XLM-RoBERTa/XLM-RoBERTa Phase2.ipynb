{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:52:27.896680Z","iopub.status.busy":"2024-05-17T10:52:27.895886Z","iopub.status.idle":"2024-05-17T10:52:46.031270Z","shell.execute_reply":"2024-05-17T10:52:46.030437Z","shell.execute_reply.started":"2024-05-17T10:52:27.896640Z"},"id":"fdl0BYLcp8lz","trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoModelForTokenClassification\n","from torch.utils.data import DataLoader , Dataset\n","from sklearn.model_selection import train_test_split\n","\n","from keras.optimizers import Adam\n","from transformers import AutoModel, BertTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments ,  AutoTokenizer\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n","from transformers.data.processors.utils import InputFeatures\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["Setting the device to CUDA"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:52:46.033589Z","iopub.status.busy":"2024-05-17T10:52:46.032903Z","iopub.status.idle":"2024-05-17T10:52:46.037943Z","shell.execute_reply":"2024-05-17T10:52:46.036823Z","shell.execute_reply.started":"2024-05-17T10:52:46.033559Z"},"trusted":true},"outputs":[],"source":["train_path = 'preprocessed_data.txt'\n","val_path = 'preprocessed_val_data.txt'"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:52:46.039654Z","iopub.status.busy":"2024-05-17T10:52:46.039267Z","iopub.status.idle":"2024-05-17T10:52:46.144728Z","shell.execute_reply":"2024-05-17T10:52:46.143654Z","shell.execute_reply.started":"2024-05-17T10:52:46.039618Z"},"id":"UiLbiqiOp8l2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["95\n"]}],"source":["df = pd.read_csv(train_path, sep=\"\\t\", names=[\"token\", \"tag\"])\n","val_df = pd.read_csv(val_path, sep=\"\\t\", names=[\"token\", \"tag\"])\n","\n","train_labels = df['tag'].unique()\n","val_labels = val_df['tag'].unique()\n","\n","all_labels = np.unique(np.concatenate((train_labels, val_labels)))\n","print(len(all_labels))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T10:52:46.147430Z","iopub.status.busy":"2024-05-17T10:52:46.147081Z","iopub.status.idle":"2024-05-17T10:52:46.165698Z","shell.execute_reply":"2024-05-17T10:52:46.164641Z","shell.execute_reply.started":"2024-05-17T10:52:46.147402Z"},"id":"pbn5EymNp8l3","outputId":"36d507ec-17b0-4a28-df0a-2f9479f55df5","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>وعي</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>لاوعي</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>عم</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>شاف</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>حلم</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>50049</th>\n","      <td>بازار</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>50050</th>\n","      <td>انطلاقة</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>50051</th>\n","      <td>ضابط</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>50052</th>\n","      <td>احز</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>50053</th>\n","      <td>مواساة</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50054 rows × 2 columns</p>\n","</div>"],"text/plain":["         token tag\n","0          وعي   O\n","1        لاوعي   O\n","2           عم   O\n","3          شاف   O\n","4          حلم   O\n","...        ...  ..\n","50049    بازار   O\n","50050  انطلاقة   O\n","50051     ضابط   O\n","50052      احز   O\n","50053   مواساة   O\n","\n","[50054 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T10:52:46.167233Z","iopub.status.busy":"2024-05-17T10:52:46.166856Z","iopub.status.idle":"2024-05-17T10:52:46.204717Z","shell.execute_reply":"2024-05-17T10:52:46.203712Z","shell.execute_reply.started":"2024-05-17T10:52:46.167198Z"},"id":"HWzUGsuPp8l4","outputId":"ff38beb5-f5c5-475d-e354-1b0e32fe0671","trusted":true},"outputs":[{"data":{"text/plain":["19"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["max_length = df[\"token\"].str.len().max()\n","max_length"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T11:04:19.046065Z","iopub.status.busy":"2024-05-17T11:04:19.045696Z","iopub.status.idle":"2024-05-17T11:04:42.771571Z","shell.execute_reply":"2024-05-17T11:04:42.770647Z","shell.execute_reply.started":"2024-05-17T11:04:19.046040Z"},"id":"BBD4kE0zp8l5","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aeac84c06cc04d499871b596be960f5c","version_major":2,"version_minor":0},"text/plain":["tf_model.h5:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\KimoStore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KimoStore\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","All TF 2.0 model weights were used when initializing XLMRobertaForTokenClassification.\n","\n","All the weights of XLMRobertaForTokenClassification were initialized from the TF 2.0 model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForTokenClassification for predictions without further training.\n"]}],"source":["model_name = 'xlm-roberta-base'\n","\n","xlm_model = AutoModelForTokenClassification.from_pretrained(model_name, from_tf=True)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:52:58.414322Z","iopub.status.busy":"2024-05-17T10:52:58.412262Z","iopub.status.idle":"2024-05-17T10:52:58.422010Z","shell.execute_reply":"2024-05-17T10:52:58.421011Z","shell.execute_reply.started":"2024-05-17T10:52:58.414271Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","label_encoder.fit(all_labels)\n","train_label_encoded= label_encoder.transform(train_labels)\n","val_label_encoded= label_encoder.transform(val_labels)\n","train_labels = train_label_encoded.tolist()\n","val_labels= val_label_encoded.tolist()"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T10:52:58.423828Z","iopub.status.busy":"2024-05-17T10:52:58.423415Z","iopub.status.idle":"2024-05-17T10:52:59.842946Z","shell.execute_reply":"2024-05-17T10:52:59.841480Z","shell.execute_reply.started":"2024-05-17T10:52:58.423790Z"},"id":"MwsY_u9wp8l6","outputId":"78412ee6-2cb6-455c-dd0f-7d6d5faa2983","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Length:  95 \n","Content:  {'B-AIRPORT': 0, 'B-BOUNDARY': 1, 'B-BUILDING-OR-GROUNDS': 2, 'B-CAMP': 3, 'B-CARDINAL': 4, 'B-CELESTIAL': 5, 'B-CLUSTER': 6, 'B-COM': 7, 'B-CONTINENT': 8, 'B-COUNTRY': 9, 'B-CURR': 10, 'B-Cluster': 11, 'B-DATE': 12, 'B-EDU': 13, 'B-ENT': 14, 'B-EVENT': 15, 'B-GOV': 16, 'B-LAND': 17, 'B-LAND-REGION-NATURAL': 18, 'B-LANGUAGE': 19, 'B-LAW': 20, 'B-MED': 21, 'B-MONEY': 22, 'B-NEIGHBORHOOD': 23, 'B-NONGOV': 24, 'B-NORP': 25, 'B-OCC': 26, 'B-ORDINAL': 27, 'B-ORG': 28, 'B-PATH': 29, 'B-PERCENT': 30, 'B-PERS': 31, 'B-PLANT': 32, 'B-PRODUCT': 33, 'B-QUANTITY': 34, 'B-REGION-GENERAL': 35, 'B-REGION-INTERNATIONAL': 36, 'B-REL': 37, 'B-SCI': 38, 'B-SPO': 39, 'B-SPORT': 40, 'B-STATE-OR-PROVINCE': 41, 'B-SUBAREA-FACILITY': 42, 'B-TIME': 43, 'B-TOWN': 44, 'B-UNIT': 45, 'B-WATER-BODY': 46, 'B-WEBSITE': 47, 'I-AIRPORT': 48, 'I-BOUNDARY': 49, 'I-BUILDING-OR-GROUNDS': 50, 'I-CAMP': 51, 'I-CARDINAL': 52, 'I-CLUSTER': 53, 'I-COM': 54, 'I-CONTINENT': 55, 'I-COUNTRY': 56, 'I-CURR': 57, 'I-Cluster': 58, 'I-DATE': 59, 'I-EDU': 60, 'I-ENT': 61, 'I-EVENT': 62, 'I-GOV': 63, 'I-LAND-REGION-NATURAL': 64, 'I-LANGUAGE': 65, 'I-LAW': 66, 'I-MED': 67, 'I-MONEY': 68, 'I-NEIGHBORHOOD': 69, 'I-NONGOV': 70, 'I-NORP': 71, 'I-OCC': 72, 'I-ORDINAL': 73, 'I-ORG': 74, 'I-PATH': 75, 'I-PERCENT': 76, 'I-PERS': 77, 'I-PLANT': 78, 'I-PRODUCT': 79, 'I-Path': 80, 'I-QUANTITY': 81, 'I-REGION-GENERAL': 82, 'I-REGION-INTERNATIONAL': 83, 'I-REL': 84, 'I-SCI': 85, 'I-SPO': 86, 'I-SPORT': 87, 'I-STATE-OR-PROVINCE': 88, 'I-SUBAREA-FACILITY': 89, 'I-TIME': 90, 'I-TOWN': 91, 'I-WATER-BODY': 92, 'I-WEBSITE': 93, 'O': 94}\n"]}],"source":["label_map = {label: index for label, index in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n","print(\"Length: \",len(label_map),\"\\nContent: \",label_map)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Preparing the Dataset"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:52:59.844753Z","iopub.status.busy":"2024-05-17T10:52:59.844360Z","iopub.status.idle":"2024-05-17T10:52:59.939613Z","shell.execute_reply":"2024-05-17T10:52:59.938462Z","shell.execute_reply.started":"2024-05-17T10:52:59.844726Z"},"id":"uvNlcFtCp8l6","trusted":true},"outputs":[],"source":["class XLMDataset(Dataset):\n","    def __init__(self, data, model_name, max_len, label_map):\n","      super(XLMDataset).__init__()\n","      self.data = data\n","      self.tokenizer_name = model_name\n","      self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n","      self.max_len = max_len\n","      self.label_map = label_map\n","\n","\n","    def __len__(self):\n","      # return len(self.text)\n","      return len(self.data)\n","\n","    def __getitem__(self,item):\n","      text = str(self.data[item][0])\n","      target = self.data[item][1]\n","\n","      encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","\n","      input_ids = encoding['input_ids'].squeeze(0)\n","      attention_mask = encoding['attention_mask'].squeeze(0)\n","      label_id = self.label_map[target]\n","\n","      return{\n","         'input_ids': input_ids,\n","          'attention_mask': attention_mask,\n","            'labels': label_id\n","      }"]},{"cell_type":"markdown","metadata":{},"source":["## Dividing the dataset into training and validation"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305,"referenced_widgets":["fb4080e2efa741bc9a0bbd9383413449","6cbed1c21ccd408fab8c7afce5b9e150","698f354baad342e4af05bc8619ea49e1","03fe006d02d54a9a89928426a2e12208","eb75a9c5a974403089c098cec7346bdd","f0c061a4d7914954907bd520bbb411a0","71c081720d964736b4d268701983fa20","80a70c196088408ea9e0e0e4062ebbbd","fc627110018648709a4d49c0dd688142","2d1d832c67d5460d803d22859a3c64d9","dab91bffe43e46babc5baa9034a3c374","21d73cc840a145de95e7d5c1f84ab074","a1be38a168c34c30b39e0696a1b32878","5b05a6960b074d3b99415c16d4fe0300","82ce66c2c7754d9ca54e106b49bfba28","1db40bdaad4b4f189c0a0d92be586eaf","da2e21de8d284a5aaa795169686e0ec1","abddf954b0d6487a9944f4ba4b6b6f3c","9f99ec2024734d37b45add25518c8520","c23da2ee2f5f488b936b8fabf251747c","338eea0b96cb4c4a904d81dd55297bb0","72754cc2f786499a9a4770647eba4f7a","47bfa3e1eed247a49a8a38019d028b82","a5879d8a9bb84a0c9af04c25036e605d","7d3c981824f94de1b7f5cc2a41e39558","a201c633495d4f1abecb04ad8656cb39","1bb40d9a70744617a0ca1562ee5ce8b1","d406410c84ba4b9b9d9846fc58402a46","fbead7077a2e49939ed9d74d3be4137e","ce00396354164520935a4f5e93634444","51eb4e78c1da4a12bfa81a56ba2ae6c6","9da7af4d0b2e4a65b9865c48d744adb9","e3d5096ad19b4c73a78bfa39c042fcc2","0c0fcfc791064fd186a149033ec72266","2b9896d61aa14e679ed145ecd6009ea6","a4aceb8239d54f659e7f799567a46853","6b82dc1e041241ff9c1fa17d57beb6e9","7e52711477404fd39a375160d2f112bb","b9b970addd1a4db691cd30c117f320ce","1b03c19111164e728c988dc750cb36a5","73487e9456b443eebf920088d51f2ca2","08be7b8d59e346fd8eb51e499a0a1623","1d5c8a389ad646c98055621932b27d10","9f29db525a0c4af28e44ec4743db93ea","8f8db769f07d4f0cad0086245dce19a0","e7a8596a4ebc44b2b3a9b7610210d04f","aa9d710b95c045d4a1173ec2909b40e2","a23758bcce69442eb869217f659f3a7c","12fc25df95334f0dbefb3b6e2786cb32","7b4335859b7543438d1f8f54f2346f3b","8cbffa22fdaa43a495c747473301e11e","d9ac487e5476437bac1b3f871abb4ef5","2ba967bf427a432d8f4c35842bfaf2ad","87f74fc6dc714b2e9ec6a227594a8529","015b864417cf460fac0efb69cfbe7422"]},"execution":{"iopub.execute_input":"2024-05-17T10:52:59.946566Z","iopub.status.busy":"2024-05-17T10:52:59.945437Z","iopub.status.idle":"2024-05-17T10:53:02.790859Z","shell.execute_reply":"2024-05-17T10:53:02.789864Z","shell.execute_reply.started":"2024-05-17T10:52:59.946534Z"},"id":"FwyyHxwXp8l7","outputId":"639d92f6-7fe7-499c-dc95-a81359d54eee","trusted":true},"outputs":[],"source":["train_dataset = XLMDataset(df[['token', 'tag']].values.tolist(), model_name, max_length, label_map)"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T10:53:02.792837Z","iopub.status.busy":"2024-05-17T10:53:02.792175Z","iopub.status.idle":"2024-05-17T10:53:02.798757Z","shell.execute_reply":"2024-05-17T10:53:02.797694Z","shell.execute_reply.started":"2024-05-17T10:53:02.792807Z"},"id":"r88ZrNMap8l8","outputId":"222406e5-d8f8-41a6-9544-9b9d3efc291d","trusted":true},"outputs":[{"data":{"text/plain":["50054"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:53:02.800195Z","iopub.status.busy":"2024-05-17T10:53:02.799919Z","iopub.status.idle":"2024-05-17T10:53:05.609279Z","shell.execute_reply":"2024-05-17T10:53:05.608241Z","shell.execute_reply.started":"2024-05-17T10:53:02.800172Z"},"trusted":true},"outputs":[],"source":["val_dataset = XLMDataset(val_df[['token', 'tag']].values.tolist(), model_name, max_length, label_map)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:53:05.610830Z","iopub.status.busy":"2024-05-17T10:53:05.610534Z","iopub.status.idle":"2024-05-17T10:53:05.616920Z","shell.execute_reply":"2024-05-17T10:53:05.615915Z","shell.execute_reply.started":"2024-05-17T10:53:05.610806Z"},"trusted":true},"outputs":[{"data":{"text/plain":["14576"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["len(val_dataset)"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:53:05.654226Z","iopub.status.busy":"2024-05-17T10:53:05.653981Z","iopub.status.idle":"2024-05-17T10:53:05.666186Z","shell.execute_reply":"2024-05-17T10:53:05.665325Z","shell.execute_reply.started":"2024-05-17T10:53:05.654205Z"},"id":"1FOUPh8gp8l9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["783\n"]}],"source":["train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","print(len(train_dataloader))"]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-17T10:53:05.667597Z","iopub.status.busy":"2024-05-17T10:53:05.667197Z","iopub.status.idle":"2024-05-17T10:53:05.677012Z","shell.execute_reply":"2024-05-17T10:53:05.676076Z","shell.execute_reply.started":"2024-05-17T10:53:05.667565Z"},"id":"gF0p8wfcp8l-","outputId":"a0115978-cc92-402e-de31-9fc6e3eee8d3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["228\n"]}],"source":["val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n","print(len(val_dataloader))"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the model"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:53:05.678460Z","iopub.status.busy":"2024-05-17T10:53:05.678109Z","iopub.status.idle":"2024-05-17T10:53:05.687252Z","shell.execute_reply":"2024-05-17T10:53:05.686411Z","shell.execute_reply.started":"2024-05-17T10:53:05.678436Z"},"id":"NPcxcxoNp8l-","trusted":true},"outputs":[],"source":["learning_rate = 2e-5\n","epochs = 5 # we changed them later\n","num_classes = len(label_map)"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T11:06:50.713280Z","iopub.status.busy":"2024-05-17T11:06:50.712354Z","iopub.status.idle":"2024-05-17T11:07:00.163690Z","shell.execute_reply":"2024-05-17T11:07:00.162572Z","shell.execute_reply.started":"2024-05-17T11:06:50.713238Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model=AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))\n"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:54:19.243972Z","iopub.status.busy":"2024-05-17T10:54:19.243560Z","iopub.status.idle":"2024-05-17T10:54:19.249597Z","shell.execute_reply":"2024-05-17T10:54:19.248449Z","shell.execute_reply.started":"2024-05-17T10:54:19.243940Z"},"trusted":true},"outputs":[],"source":["def compute_accuracy(predictions, labels):\n","    predictions = torch.argmax(predictions, dim=1)\n","    correct = (predictions == labels).sum().item()\n","    total = labels.size(0)\n","    return correct / total"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the model on a small subset"]},{"cell_type":"markdown","metadata":{},"source":["We used here mixed precision technique to speed the training process"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["subset_size = 1000  # Set the desired size of the subset\n","train_dataset_subset = torch.utils.data.Subset(train_dataset, range(subset_size))\n","subset_train_dataloader = DataLoader(train_dataset_subset, batch_size=64, shuffle=True)\n","\n","val_dataset_subset = torch.utils.data.Subset(val_dataset, range(subset_size))\n","subset_val_dataloader = DataLoader(train_dataset_subset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\KimoStore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 0.1 ----> Training Loss: 0.1245, Training Accuracy: 0.9625 Validation Loss: 0.5432, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 1.1 ----> Training Loss: 0.1019, Training Accuracy: 0.9625 Validation Loss: 0.5203, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 2.1 ----> Training Loss: 0.0902, Training Accuracy: 0.9625 Validation Loss: 0.5198, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 3.1 ----> Training Loss: 0.0843, Training Accuracy: 0.9625 Validation Loss: 0.5240, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 4.1 ----> Training Loss: 0.0827, Training Accuracy: 0.9625 Validation Loss: 0.5394, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 5.1 ----> Training Loss: 0.0808, Training Accuracy: 0.9625 Validation Loss: 0.5433, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 6.1 ----> Training Loss: 0.0790, Training Accuracy: 0.9625 Validation Loss: 0.5453, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 7.1 ----> Training Loss: 0.0802, Training Accuracy: 0.9625 Validation Loss: 0.5264, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 8.1 ----> Training Loss: 0.0807, Training Accuracy: 0.9625 Validation Loss: 0.5189, Validation Accuracy: 0.9350\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    "]},{"name":"stdout","output_type":"stream","text":["Epoch number: 9.1 ----> Training Loss: 0.0790, Training Accuracy: 0.9625 Validation Loss: 0.5257, Validation Accuracy: 0.9350\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","accumulation_steps = 4 \n","accumulated_batch_count = 0\n","\n","epochs  = 10\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_acc = 0.0\n","    train_samples = 0\n","\n","    print(\"-------------------------------------------------------\")\n","    with tqdm(total=len(subset_train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as pbar:\n","        with autocast():\n","            for batch in subset_train_dataloader:\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = (outputs.loss) / accumulation_steps\n","                logits = outputs.logits\n","                loss.backward()\n","                accumulated_batch_count += 1\n","                # optimizer.step()\n","                if accumulated_batch_count % accumulation_steps == 0:\n","                    # Perform optimizer step and reset accumulated_batch_count\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    accumulated_batch_count = 0  # Reset accumulated_batch_count\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_train_loss += loss.item() * labels.size(0)\n","                total_train_acc += acc * labels.size(0)\n","                train_samples += labels.size(0)\n","                pbar.update(1)\n","\n","        #  Final optimizer step for remaining accumulated gradients\n","        if accumulated_batch_count > 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            accumulated_batch_count = 0\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    total_val_loss = 0.0\n","    total_val_acc = 0.0\n","    val_samples = 0\n","\n","    with tqdm(total=len(subset_val_dataloader), desc=f\"Epoch {epoch+1} (validation)\", leave=False) as pbar:\n","        for batch in subset_val_dataloader:\n","            with autocast():\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                with torch.no_grad():\n","                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_val_loss += loss.item() * labels.size(0)\n","                total_val_acc += acc * labels.size(0)\n","                val_samples += labels.size(0)\n","                pbar.update(1)\n","\n","    train_loss = total_train_loss / train_samples\n","    train_acc = total_train_acc / train_samples\n","    val_loss = total_val_loss / val_samples\n","    val_acc = total_val_acc / val_samples\n","\n","    print(f\"Epoch number: {epoch+1 / epochs} ----> \"\n","          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f} \"\n","          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# **Training the entire dataset**"]},{"cell_type":"markdown","metadata":{},"source":["Here we trained the dataset using samples dump and load method, but dividing the dataset into samples, train and evaluate the model, then store the model into a pickle file for later training"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["train_subset_size = 5000  # Set the desired size of the subset\n","train_subset = torch.utils.data.Subset(train_dataset, range(train_subset_size))\n","subset_train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n","\n","val_subset_size = 1000\n","val_subset = torch.utils.data.Subset(val_dataset, range(val_subset_size))\n","subset_val_dataloader = DataLoader(val_subset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"data":{"text/plain":["5000"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["len(train_subset)"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/plain":["1000"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["len(val_subset)"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\KimoStore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 0.2 ----> Training Loss: 1.2618, Training Accuracy: 0.0000 Validation Loss: 3.9749, Validation Accuracy: 0.0000\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 1.2 ----> Training Loss: 0.6378, Training Accuracy: 0.5144 Validation Loss: 0.8998, Validation Accuracy: 0.9390\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 2.2 ----> Training Loss: 0.1938, Training Accuracy: 0.9322 Validation Loss: 0.5089, Validation Accuracy: 0.9390\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 3.2 ----> Training Loss: 0.1436, Training Accuracy: 0.9322 Validation Loss: 0.4988, Validation Accuracy: 0.9390\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     "]},{"name":"stdout","output_type":"stream","text":["Epoch number: 4.2 ----> Training Loss: 0.1373, Training Accuracy: 0.9322 Validation Loss: 0.4954, Validation Accuracy: 0.9390\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","accumulation_steps = 4 \n","accumulated_batch_count = 0\n","\n","epochs  = 5\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_acc = 0.0\n","    train_samples = 0\n","\n","    print(\"-------------------------------------------------------\")\n","    with tqdm(total=len(subset_train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as pbar:\n","        with autocast():\n","            for batch in subset_train_dataloader:\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = (outputs.loss) / accumulation_steps\n","                logits = outputs.logits\n","                loss.backward()\n","                accumulated_batch_count += 1\n","                # optimizer.step()\n","                if accumulated_batch_count % accumulation_steps == 0:\n","                    # Perform optimizer step and reset accumulated_batch_count\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    accumulated_batch_count = 0  # Reset accumulated_batch_count\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_train_loss += loss.item() * labels.size(0)\n","                total_train_acc += acc * labels.size(0)\n","                train_samples += labels.size(0)\n","                pbar.update(1)\n","\n","        #  Final optimizer step for remaining accumulated gradients\n","        if accumulated_batch_count > 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            accumulated_batch_count = 0\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    total_val_loss = 0.0\n","    total_val_acc = 0.0\n","    val_samples = 0\n","\n","    with tqdm(total=len(subset_val_dataloader), desc=f\"Epoch {epoch+1} (validation)\", leave=False) as pbar:\n","        for batch in subset_val_dataloader:\n","            with autocast():\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                with torch.no_grad():\n","                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_val_loss += loss.item() * labels.size(0)\n","                total_val_acc += acc * labels.size(0)\n","                val_samples += labels.size(0)\n","                pbar.update(1)\n","\n","    train_loss = total_train_loss / train_samples\n","    train_acc = total_train_acc / train_samples\n","    val_loss = total_val_loss / val_samples\n","    val_acc = total_val_acc / val_samples\n","\n","    print(f\"Epoch number: {epoch+1 / epochs} ----> \"\n","          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f} \"\n","          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["import pickle\n","\n","with open('xlm_model_5000.pkl', 'wb') as f:\n","    pickle.dump(model, f)"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["with open('xlm_model_5000.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5000\n","1000\n","3\n"]}],"source":["i = 2\n","train_subset = torch.utils.data.Subset(train_dataset, range(train_subset_size, i*train_subset_size))\n","subset_train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n","\n","val_subset = torch.utils.data.Subset(val_dataset, range(val_subset_size, i*val_subset_size))\n","subset_val_dataloader = DataLoader(val_subset, batch_size=64, shuffle=True)\n","\n","i+=1\n","\n","print(len(train_subset))\n","print(len(val_subset))\n","print(i)"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\KimoStore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 0.2 ----> Training Loss: 0.1383, Training Accuracy: 0.9310 Validation Loss: 0.7253, Validation Accuracy: 0.9070\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 1.2 ----> Training Loss: 0.1376, Training Accuracy: 0.9310 Validation Loss: 0.7253, Validation Accuracy: 0.9070\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 2.2 ----> Training Loss: 0.1378, Training Accuracy: 0.9310 Validation Loss: 0.7253, Validation Accuracy: 0.9070\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 3.2 ----> Training Loss: 0.1378, Training Accuracy: 0.9310 Validation Loss: 0.7253, Validation Accuracy: 0.9070\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     "]},{"name":"stdout","output_type":"stream","text":["Epoch number: 4.2 ----> Training Loss: 0.1380, Training Accuracy: 0.9310 Validation Loss: 0.7253, Validation Accuracy: 0.9070\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","accumulation_steps = 4 \n","accumulated_batch_count = 0\n","\n","model = loaded_model\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_acc = 0.0\n","    train_samples = 0\n","\n","    print(\"-------------------------------------------------------\")\n","    with tqdm(total=len(subset_train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as pbar:\n","        with autocast():\n","            for batch in subset_train_dataloader:\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = (outputs.loss) / accumulation_steps\n","                logits = outputs.logits\n","                loss.backward()\n","                accumulated_batch_count += 1\n","                # optimizer.step()\n","                if accumulated_batch_count % accumulation_steps == 0:\n","                    # Perform optimizer step and reset accumulated_batch_count\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    accumulated_batch_count = 0  # Reset accumulated_batch_count\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_train_loss += loss.item() * labels.size(0)\n","                total_train_acc += acc * labels.size(0)\n","                train_samples += labels.size(0)\n","                pbar.update(1)\n","\n","        #  Final optimizer step for remaining accumulated gradients\n","        if accumulated_batch_count > 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            accumulated_batch_count = 0\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    total_val_loss = 0.0\n","    total_val_acc = 0.0\n","    val_samples = 0\n","\n","    with tqdm(total=len(subset_val_dataloader), desc=f\"Epoch {epoch+1} (validation)\", leave=False) as pbar:\n","        for batch in subset_val_dataloader:\n","            with autocast():\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                with torch.no_grad():\n","                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_val_loss += loss.item() * labels.size(0)\n","                total_val_acc += acc * labels.size(0)\n","                val_samples += labels.size(0)\n","                pbar.update(1)\n","\n","    train_loss = total_train_loss / train_samples\n","    train_acc = total_train_acc / train_samples\n","    val_loss = total_val_loss / val_samples\n","    val_acc = total_val_acc / val_samples\n","\n","    print(f\"Epoch number: {epoch+1 / epochs} ----> \"\n","          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f} \"\n","          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["import pickle\n","\n","with open('xlm_model_10000.pkl', 'wb') as f:\n","    pickle.dump(model, f)"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["with open('xlm_model_10000.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n","2000\n","3\n"]}],"source":["\n","train_subset = torch.utils.data.Subset(train_dataset, range(10000, 20000))\n","subset_train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n","\n","val_subset = torch.utils.data.Subset(val_dataset, range(2000, 4000))\n","subset_val_dataloader = DataLoader(val_subset, batch_size=64, shuffle=True)\n","\n","print(len(train_subset))\n","print(len(val_subset))\n","print(i)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\KimoStore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 1/5 ---> Training Loss: 0.5592, Training Accuracy: 0.6901 Validation Loss: 2.0605, Validation Accuracy: 0.7310\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 2/5 ---> Training Loss: 0.5584, Training Accuracy: 0.6901 Validation Loss: 2.0605, Validation Accuracy: 0.7310\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 3/5 ---> Training Loss: 0.5576, Training Accuracy: 0.6901 Validation Loss: 2.0605, Validation Accuracy: 0.7310\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 4/5 ---> Training Loss: 0.5580, Training Accuracy: 0.6901 Validation Loss: 2.0605, Validation Accuracy: 0.7310\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     "]},{"name":"stdout","output_type":"stream","text":["Epoch number: 5/5 ---> Training Loss: 0.5581, Training Accuracy: 0.6901 Validation Loss: 2.0605, Validation Accuracy: 0.7310\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","accumulation_steps = 4 \n","accumulated_batch_count = 0\n","\n","model = loaded_model\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_acc = 0.0\n","    train_samples = 0\n","\n","    print(\"-------------------------------------------------------\")\n","    with tqdm(total=len(subset_train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as pbar:\n","        with autocast():\n","            for batch in subset_train_dataloader:\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = (outputs.loss) / accumulation_steps\n","                logits = outputs.logits\n","                loss.backward()\n","                accumulated_batch_count += 1\n","                # optimizer.step()\n","                if accumulated_batch_count % accumulation_steps == 0:\n","                    # Perform optimizer step and reset accumulated_batch_count\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    accumulated_batch_count = 0  # Reset accumulated_batch_count\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_train_loss += loss.item() * labels.size(0)\n","                total_train_acc += acc * labels.size(0)\n","                train_samples += labels.size(0)\n","                pbar.update(1)\n","\n","        #  Final optimizer step for remaining accumulated gradients\n","        if accumulated_batch_count > 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            accumulated_batch_count = 0\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    total_val_loss = 0.0\n","    total_val_acc = 0.0\n","    val_samples = 0\n","\n","    with tqdm(total=len(subset_val_dataloader), desc=f\"Epoch {epoch+1} (validation)\", leave=False) as pbar:\n","        for batch in subset_val_dataloader:\n","            with autocast():\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                with torch.no_grad():\n","                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_val_loss += loss.item() * labels.size(0)\n","                total_val_acc += acc * labels.size(0)\n","                val_samples += labels.size(0)\n","                pbar.update(1)\n","\n","    train_loss = total_train_loss / train_samples\n","    train_acc = total_train_acc / train_samples\n","    val_loss = total_val_loss / val_samples\n","    val_acc = total_val_acc / val_samples\n","\n","    print(f\"Epoch number: {epoch+1}/{epochs} ---> \"\n","          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f} \"\n","          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["import pickle\n","\n","with open('xlm_model_20000.pkl', 'wb') as f:\n","    pickle.dump(model, f)"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["with open('xlm_model_20000.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n","3000\n","3\n"]}],"source":["\n","train_subset = torch.utils.data.Subset(train_dataset, range(20000, 30000))\n","subset_train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n","\n","val_subset = torch.utils.data.Subset(val_dataset, range(4000, 7000))\n","subset_val_dataloader = DataLoader(val_subset, batch_size=64, shuffle=True)\n","\n","print(len(train_subset))\n","print(len(val_subset))\n","print(i)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\KimoStore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 1/3 ---> Training Loss: 0.6805, Training Accuracy: 0.6234 Validation Loss: 3.5797, Validation Accuracy: 0.5323\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 2/3 ---> Training Loss: 0.6813, Training Accuracy: 0.6234 Validation Loss: 3.5797, Validation Accuracy: 0.5323\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     "]},{"name":"stdout","output_type":"stream","text":["Epoch number: 3/3 ---> Training Loss: 0.6817, Training Accuracy: 0.6233 Validation Loss: 3.5797, Validation Accuracy: 0.5323\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","epochs = 3\n","\n","accumulation_steps = 4 \n","accumulated_batch_count = 0\n","\n","model = loaded_model\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_acc = 0.0\n","    train_samples = 0\n","\n","    print(\"-------------------------------------------------------\")\n","    with tqdm(total=len(subset_train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as pbar:\n","        with autocast():\n","            for batch in subset_train_dataloader:\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = (outputs.loss) / accumulation_steps\n","                logits = outputs.logits\n","                loss.backward()\n","                accumulated_batch_count += 1\n","                # optimizer.step()\n","                if accumulated_batch_count % accumulation_steps == 0:\n","                    # Perform optimizer step and reset accumulated_batch_count\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    accumulated_batch_count = 0  # Reset accumulated_batch_count\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_train_loss += loss.item() * labels.size(0)\n","                total_train_acc += acc * labels.size(0)\n","                train_samples += labels.size(0)\n","                pbar.update(1)\n","\n","        #  Final optimizer step for remaining accumulated gradients\n","        if accumulated_batch_count > 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            accumulated_batch_count = 0\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    total_val_loss = 0.0\n","    total_val_acc = 0.0\n","    val_samples = 0\n","\n","    with tqdm(total=len(subset_val_dataloader), desc=f\"Epoch {epoch+1} (validation)\", leave=False) as pbar:\n","        for batch in subset_val_dataloader:\n","            with autocast():\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                with torch.no_grad():\n","                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_val_loss += loss.item() * labels.size(0)\n","                total_val_acc += acc * labels.size(0)\n","                val_samples += labels.size(0)\n","                pbar.update(1)\n","\n","    train_loss = total_train_loss / train_samples\n","    train_acc = total_train_acc / train_samples\n","    val_loss = total_val_loss / val_samples\n","    val_acc = total_val_acc / val_samples\n","\n","    print(f\"Epoch number: {epoch+1}/{epochs} ---> \"\n","          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f} \"\n","          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["import pickle\n","\n","with open('xlm_model_30000.pkl', 'wb') as f:\n","    pickle.dump(model, f)"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["with open('xlm_model_30000.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10000\n","3000\n","3\n"]}],"source":["\n","train_subset = torch.utils.data.Subset(train_dataset, range(30000, 40000))\n","subset_train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n","\n","val_subset = torch.utils.data.Subset(val_dataset, range(7000, 10000))\n","subset_val_dataloader = DataLoader(val_subset, batch_size=64, shuffle=True)\n","\n","print(len(train_subset))\n","print(len(val_subset))\n","print(i)"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 1/3 ---> Training Loss: 0.4874, Training Accuracy: 0.7315 Validation Loss: 1.9389, Validation Accuracy: 0.7493\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 2/3 ---> Training Loss: 0.4886, Training Accuracy: 0.7315 Validation Loss: 1.9389, Validation Accuracy: 0.7493\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     "]},{"name":"stdout","output_type":"stream","text":["Epoch number: 3/3 ---> Training Loss: 0.4877, Training Accuracy: 0.7315 Validation Loss: 1.9389, Validation Accuracy: 0.7493\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","epochs = 3\n","\n","accumulation_steps = 4 \n","accumulated_batch_count = 0\n","\n","model = loaded_model\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_acc = 0.0\n","    train_samples = 0\n","\n","    print(\"-------------------------------------------------------\")\n","    with tqdm(total=len(subset_train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as pbar:\n","        with autocast():\n","            for batch in subset_train_dataloader:\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = (outputs.loss) / accumulation_steps\n","                logits = outputs.logits\n","                loss.backward()\n","                accumulated_batch_count += 1\n","                # optimizer.step()\n","                if accumulated_batch_count % accumulation_steps == 0:\n","                    # Perform optimizer step and reset accumulated_batch_count\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    accumulated_batch_count = 0  # Reset accumulated_batch_count\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_train_loss += loss.item() * labels.size(0)\n","                total_train_acc += acc * labels.size(0)\n","                train_samples += labels.size(0)\n","                pbar.update(1)\n","\n","        #  Final optimizer step for remaining accumulated gradients\n","        if accumulated_batch_count > 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            accumulated_batch_count = 0\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    total_val_loss = 0.0\n","    total_val_acc = 0.0\n","    val_samples = 0\n","\n","    with tqdm(total=len(subset_val_dataloader), desc=f\"Epoch {epoch+1} (validation)\", leave=False) as pbar:\n","        for batch in subset_val_dataloader:\n","            with autocast():\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                with torch.no_grad():\n","                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_val_loss += loss.item() * labels.size(0)\n","                total_val_acc += acc * labels.size(0)\n","                val_samples += labels.size(0)\n","                pbar.update(1)\n","\n","    train_loss = total_train_loss / train_samples\n","    train_acc = total_train_acc / train_samples\n","    val_loss = total_val_loss / val_samples\n","    val_acc = total_val_acc / val_samples\n","\n","    print(f\"Epoch number: {epoch+1}/{epochs} ---> \"\n","          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f} \"\n","          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["import pickle\n","\n","with open('xlm_model_40000.pkl', 'wb') as f:\n","    pickle.dump(model, f)"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["with open('xlm_model_40000.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10054\n","4576\n"]}],"source":["\n","train_subset = torch.utils.data.Subset(train_dataset, range(40000, 50054))\n","subset_train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)\n","\n","val_subset = torch.utils.data.Subset(val_dataset, range(10000, 14576))\n","subset_val_dataloader = DataLoader(val_subset, batch_size=64, shuffle=True)\n","\n","print(len(train_subset))\n","print(len(val_subset))"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 1/3 ---> Training Loss: 0.5479, Training Accuracy: 0.6972 Validation Loss: 1.9979, Validation Accuracy: 0.7399\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Epoch number: 2/3 ---> Training Loss: 0.5464, Training Accuracy: 0.6972 Validation Loss: 1.9979, Validation Accuracy: 0.7399\n","-------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     "]},{"name":"stdout","output_type":"stream","text":["Epoch number: 3/3 ---> Training Loss: 0.5477, Training Accuracy: 0.6972 Validation Loss: 1.9979, Validation Accuracy: 0.7399\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from transformers import AdamW, get_scheduler\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","epochs = 3\n","\n","accumulation_steps = 4 \n","accumulated_batch_count = 0\n","\n","model = loaded_model\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_train_loss = 0.0\n","    total_train_acc = 0.0\n","    train_samples = 0\n","\n","    print(\"-------------------------------------------------------\")\n","    with tqdm(total=len(subset_train_dataloader), desc=f\"Epoch {epoch+1}\", leave=False) as pbar:\n","        with autocast():\n","            for batch in subset_train_dataloader:\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = (outputs.loss) / accumulation_steps\n","                logits = outputs.logits\n","                loss.backward()\n","                accumulated_batch_count += 1\n","                # optimizer.step()\n","                if accumulated_batch_count % accumulation_steps == 0:\n","                    # Perform optimizer step and reset accumulated_batch_count\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    accumulated_batch_count = 0  # Reset accumulated_batch_count\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_train_loss += loss.item() * labels.size(0)\n","                total_train_acc += acc * labels.size(0)\n","                train_samples += labels.size(0)\n","                pbar.update(1)\n","\n","        #  Final optimizer step for remaining accumulated gradients\n","        if accumulated_batch_count > 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            accumulated_batch_count = 0\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    total_val_loss = 0.0\n","    total_val_acc = 0.0\n","    val_samples = 0\n","\n","    with tqdm(total=len(subset_val_dataloader), desc=f\"Epoch {epoch+1} (validation)\", leave=False) as pbar:\n","        for batch in subset_val_dataloader:\n","            with autocast():\n","                input_ids = batch['input_ids']\n","                attention_mask = batch['attention_mask']\n","                labels = batch['labels'].long()\n","\n","                with torch.no_grad():\n","                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","\n","                acc = compute_accuracy(logits, labels)\n","\n","                total_val_loss += loss.item() * labels.size(0)\n","                total_val_acc += acc * labels.size(0)\n","                val_samples += labels.size(0)\n","                pbar.update(1)\n","\n","    train_loss = total_train_loss / train_samples\n","    train_acc = total_train_acc / train_samples\n","    val_loss = total_val_loss / val_samples\n","    val_acc = total_val_acc / val_samples\n","\n","    print(f\"Epoch number: {epoch+1}/{epochs} ---> \"\n","          f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f} \"\n","          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["import pickle\n","\n","with open('xlm_model_full.pkl', 'wb') as f:\n","    pickle.dump(model, f)"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["with open('xlm_model_full.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["# **Model Evaluation:**"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                   "]},{"name":"stdout","output_type":"stream","text":["Precision: 0.5228, Recall: 0.7230, F1-score: 0.6068\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\KimoStore\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import precision_recall_fscore_support\n","\n","model.eval()\n","all_logits = []\n","all_labels = []\n","\n","with tqdm(total=len(val_dataloader), desc=f\"Evaluating model\", leave=False) as pbar:\n","    for batch in val_dataloader:\n","        with torch.no_grad():\n","            input_ids = batch['input_ids']\n","            attention_mask = batch['attention_mask']\n","            labels = batch['labels'].long()\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","\n","            all_logits.append(logits.detach().cpu())\n","            all_labels.append(labels.detach().cpu())\n","            pbar.update(1)\n","\n","all_logits = torch.cat(all_logits, dim=0)\n","all_labels = torch.cat(all_labels, dim=0)\n","\n","predictions = torch.argmax(all_logits, dim=-1)\n","\n","precision, recall, f1, _ = precision_recall_fscore_support(all_labels, predictions, average='weighted')\n","\n","print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5025426,"sourceId":8437071,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"015b864417cf460fac0efb69cfbe7422":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"037ff89cc5154071b49aab569b339870":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51844c5af0e44c4db5e937f7f89c5873","max":543432324,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e44844547d2747eeb29bb6e1023b9c1e","value":543432324}},"03fe006d02d54a9a89928426a2e12208":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d1d832c67d5460d803d22859a3c64d9","placeholder":"​","style":"IPY_MODEL_dab91bffe43e46babc5baa9034a3c374","value":" 381/381 [00:00&lt;00:00, 18.8kB/s]"}},"08be7b8d59e346fd8eb51e499a0a1623":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c0fcfc791064fd186a149033ec72266":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b9896d61aa14e679ed145ecd6009ea6","IPY_MODEL_a4aceb8239d54f659e7f799567a46853","IPY_MODEL_6b82dc1e041241ff9c1fa17d57beb6e9"],"layout":"IPY_MODEL_7e52711477404fd39a375160d2f112bb"}},"12fc25df95334f0dbefb3b6e2786cb32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15b552363eb84a95aa9f9fe7989715f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"174f98d3538846b7b12b81bcebf486de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b03c19111164e728c988dc750cb36a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bb40d9a70744617a0ca1562ee5ce8b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c091101c8674e23aba159989a991ccc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c73300df288c4c2994705a36c97256e4","placeholder":"​","style":"IPY_MODEL_21f3d5fcadad4fe59069ad7e118edf6c","value":" 543M/543M [00:03&lt;00:00, 148MB/s]"}},"1d5c8a389ad646c98055621932b27d10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db40bdaad4b4f189c0a0d92be586eaf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"205627fe4af149809cf91b3f649bc043":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b13d172e171427687713f5445552712","IPY_MODEL_037ff89cc5154071b49aab569b339870","IPY_MODEL_1c091101c8674e23aba159989a991ccc"],"layout":"IPY_MODEL_15b552363eb84a95aa9f9fe7989715f9"}},"21d73cc840a145de95e7d5c1f84ab074":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1be38a168c34c30b39e0696a1b32878","IPY_MODEL_5b05a6960b074d3b99415c16d4fe0300","IPY_MODEL_82ce66c2c7754d9ca54e106b49bfba28"],"layout":"IPY_MODEL_1db40bdaad4b4f189c0a0d92be586eaf"}},"21f3d5fcadad4fe59069ad7e118edf6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b9896d61aa14e679ed145ecd6009ea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9b970addd1a4db691cd30c117f320ce","placeholder":"​","style":"IPY_MODEL_1b03c19111164e728c988dc750cb36a5","value":"tokenizer.json: 100%"}},"2ba967bf427a432d8f4c35842bfaf2ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d1d832c67d5460d803d22859a3c64d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"338eea0b96cb4c4a904d81dd55297bb0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47bfa3e1eed247a49a8a38019d028b82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5879d8a9bb84a0c9af04c25036e605d","IPY_MODEL_7d3c981824f94de1b7f5cc2a41e39558","IPY_MODEL_a201c633495d4f1abecb04ad8656cb39"],"layout":"IPY_MODEL_1bb40d9a70744617a0ca1562ee5ce8b1"}},"51844c5af0e44c4db5e937f7f89c5873":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51eb4e78c1da4a12bfa81a56ba2ae6c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b05a6960b074d3b99415c16d4fe0300":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f99ec2024734d37b45add25518c8520","max":384,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c23da2ee2f5f488b936b8fabf251747c","value":384}},"698f354baad342e4af05bc8619ea49e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_80a70c196088408ea9e0e0e4062ebbbd","max":381,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc627110018648709a4d49c0dd688142","value":381}},"6b13d172e171427687713f5445552712":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_174f98d3538846b7b12b81bcebf486de","placeholder":"​","style":"IPY_MODEL_de0317406f72411fa8c621238a3543b0","value":"model.safetensors: 100%"}},"6b82dc1e041241ff9c1fa17d57beb6e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d5c8a389ad646c98055621932b27d10","placeholder":"​","style":"IPY_MODEL_9f29db525a0c4af28e44ec4743db93ea","value":" 2.64M/2.64M [00:00&lt;00:00, 10.1MB/s]"}},"6cbed1c21ccd408fab8c7afce5b9e150":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0c061a4d7914954907bd520bbb411a0","placeholder":"​","style":"IPY_MODEL_71c081720d964736b4d268701983fa20","value":"tokenizer_config.json: 100%"}},"71c081720d964736b4d268701983fa20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72754cc2f786499a9a4770647eba4f7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73487e9456b443eebf920088d51f2ca2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b4335859b7543438d1f8f54f2346f3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d3c981824f94de1b7f5cc2a41e39558":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce00396354164520935a4f5e93634444","max":824793,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51eb4e78c1da4a12bfa81a56ba2ae6c6","value":824793}},"7e52711477404fd39a375160d2f112bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80a70c196088408ea9e0e0e4062ebbbd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82ce66c2c7754d9ca54e106b49bfba28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_338eea0b96cb4c4a904d81dd55297bb0","placeholder":"​","style":"IPY_MODEL_72754cc2f786499a9a4770647eba4f7a","value":" 384/384 [00:00&lt;00:00, 23.1kB/s]"}},"87f74fc6dc714b2e9ec6a227594a8529":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cbffa22fdaa43a495c747473301e11e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f8db769f07d4f0cad0086245dce19a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7a8596a4ebc44b2b3a9b7610210d04f","IPY_MODEL_aa9d710b95c045d4a1173ec2909b40e2","IPY_MODEL_a23758bcce69442eb869217f659f3a7c"],"layout":"IPY_MODEL_12fc25df95334f0dbefb3b6e2786cb32"}},"9da7af4d0b2e4a65b9865c48d744adb9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f29db525a0c4af28e44ec4743db93ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f99ec2024734d37b45add25518c8520":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1be38a168c34c30b39e0696a1b32878":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da2e21de8d284a5aaa795169686e0ec1","placeholder":"​","style":"IPY_MODEL_abddf954b0d6487a9944f4ba4b6b6f3c","value":"config.json: 100%"}},"a201c633495d4f1abecb04ad8656cb39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9da7af4d0b2e4a65b9865c48d744adb9","placeholder":"​","style":"IPY_MODEL_e3d5096ad19b4c73a78bfa39c042fcc2","value":" 825k/825k [00:00&lt;00:00, 3.60MB/s]"}},"a23758bcce69442eb869217f659f3a7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87f74fc6dc714b2e9ec6a227594a8529","placeholder":"​","style":"IPY_MODEL_015b864417cf460fac0efb69cfbe7422","value":" 112/112 [00:00&lt;00:00, 6.93kB/s]"}},"a4aceb8239d54f659e7f799567a46853":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73487e9456b443eebf920088d51f2ca2","max":2642362,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08be7b8d59e346fd8eb51e499a0a1623","value":2642362}},"a5879d8a9bb84a0c9af04c25036e605d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d406410c84ba4b9b9d9846fc58402a46","placeholder":"​","style":"IPY_MODEL_fbead7077a2e49939ed9d74d3be4137e","value":"vocab.txt: 100%"}},"aa9d710b95c045d4a1173ec2909b40e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9ac487e5476437bac1b3f871abb4ef5","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ba967bf427a432d8f4c35842bfaf2ad","value":112}},"abddf954b0d6487a9944f4ba4b6b6f3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9b970addd1a4db691cd30c117f320ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c23da2ee2f5f488b936b8fabf251747c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c73300df288c4c2994705a36c97256e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce00396354164520935a4f5e93634444":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d406410c84ba4b9b9d9846fc58402a46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9ac487e5476437bac1b3f871abb4ef5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da2e21de8d284a5aaa795169686e0ec1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dab91bffe43e46babc5baa9034a3c374":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de0317406f72411fa8c621238a3543b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3d5096ad19b4c73a78bfa39c042fcc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e44844547d2747eeb29bb6e1023b9c1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7a8596a4ebc44b2b3a9b7610210d04f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b4335859b7543438d1f8f54f2346f3b","placeholder":"​","style":"IPY_MODEL_8cbffa22fdaa43a495c747473301e11e","value":"special_tokens_map.json: 100%"}},"eb75a9c5a974403089c098cec7346bdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0c061a4d7914954907bd520bbb411a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4080e2efa741bc9a0bbd9383413449":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6cbed1c21ccd408fab8c7afce5b9e150","IPY_MODEL_698f354baad342e4af05bc8619ea49e1","IPY_MODEL_03fe006d02d54a9a89928426a2e12208"],"layout":"IPY_MODEL_eb75a9c5a974403089c098cec7346bdd"}},"fbead7077a2e49939ed9d74d3be4137e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc627110018648709a4d49c0dd688142":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
